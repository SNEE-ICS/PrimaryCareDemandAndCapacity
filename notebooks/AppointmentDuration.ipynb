{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appointment Duration \n",
    "\n",
    "###  Aim: Make an approximation of appointment time by ICB and staff type that we can use in discrete event simulation?\n",
    "- Find all binned appointments by area\n",
    "- Fit a probability distribution function\n",
    "- Export to Yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a bit of a hack to get relative imports \n",
    "# to work as if these notebooks were in a package\n",
    "# change cwd to project root if 'notebooks' in PATH\n",
    "from os import chdir\n",
    "from pathlib import Path\n",
    "if 'notebooks' in str(Path.cwd()):\n",
    "    chdir('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# custom imports from src\n",
    "from src.schemas import DataCatalog\n",
    "import src.constants as const\n",
    "from src.various_methods import PlotCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_APPOINTMENTS_CATALOG_NAME:str = 'Appointments in General Practice, September 2023'\n",
    "HIST_BIN_EDGES = [1, 6, 11, 16, 21, 31, 60]\n",
    "OUTPUT_YAML_FILE = \"outputs/assumptions/appointment_duration.yaml\"\n",
    "NOTEBOOK_ALIAS = 'appointment_duration'\n",
    "\n",
    "plot_counter = PlotCounter(name='NOTEBOOK_ALIAS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from data catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_catalog = DataCatalog.load_from_yaml(\"data_catalog.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling\n",
    "### Part 1 - Cleaning\n",
    "1. remove unknown appointments\n",
    "2. Convert date columns\n",
    "3. Remove unrequired columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the appointments data\n",
    "appointments_catalog_entry = data_catalog.get_catalog_entry_by_name(GP_APPOINTMENTS_CATALOG_NAME)\n",
    "appointments_df = appointments_catalog_entry.load()\n",
    "html_appts =  appointments_df.head(10).to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>SUB_ICB_LOCATION_CODE</th>\\n      <th>SUB_ICB_LOCATION_ONS_CODE</th>\\n      <th>SUB_ICB_LOCATION_NAME</th>\\n      <th>ICB_ONS_CODE</th>\\n      <th>REGION_ONS_CODE</th>\\n      <th>Appointment_Date</th>\\n      <th>ACTUAL_DURATION</th>\\n      <th>COUNT_OF_APPOINTMENTS</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>01DEC2021</td>\\n      <td>1-5 Minutes</td>\\n      <td>1539</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>01DEC2021</td>\\n      <td>31-60 Minutes</td>\\n      <td>364</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>01DEC2021</td>\\n      <td>Unknown / Data Quality</td>\\n      <td>1277</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>01DEC2021</td>\\n      <td>16-20 Minutes</td>\\n      <td>730</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>01DEC2021</td>\\n      <td>11-15 Minutes</td>\\n      <td>1073</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>01DEC2021</td>\\n      <td>6-10 Minutes</td>\\n      <td>1698</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>01DEC2021</td>\\n      <td>21-30 Minutes</td>\\n      <td>619</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>02DEC2021</td>\\n      <td>6-10 Minutes</td>\\n      <td>1578</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>02DEC2021</td>\\n      <td>Unknown / Data Quality</td>\\n      <td>1391</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>00L</td>\\n      <td>E38000130</td>\\n      <td>NHS North East and North Cumbria ICB - 00L</td>\\n      <td>E54000050</td>\\n      <td>E40000012</td>\\n      <td>02DEC2021</td>\\n      <td>21-30 Minutes</td>\\n      <td>601</td>\\n    </tr>\\n  </tbody>\\n</table>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_appts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTUAL_DURATION</th>\n",
       "      <th>COUNT_OF_APPOINTMENTS</th>\n",
       "      <th>Alliance</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195134</th>\n",
       "      <td>6-10 Minutes</td>\n",
       "      <td>1825</td>\n",
       "      <td>Ipswich &amp; East Suffolk</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195135</th>\n",
       "      <td>21-30 Minutes</td>\n",
       "      <td>752</td>\n",
       "      <td>Ipswich &amp; East Suffolk</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195136</th>\n",
       "      <td>16-20 Minutes</td>\n",
       "      <td>786</td>\n",
       "      <td>Ipswich &amp; East Suffolk</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195137</th>\n",
       "      <td>31-60 Minutes</td>\n",
       "      <td>456</td>\n",
       "      <td>Ipswich &amp; East Suffolk</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195138</th>\n",
       "      <td>11-15 Minutes</td>\n",
       "      <td>1253</td>\n",
       "      <td>Ipswich &amp; East Suffolk</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACTUAL_DURATION  COUNT_OF_APPOINTMENTS                Alliance  \\\n",
       "195134    6-10 Minutes                   1825  Ipswich & East Suffolk   \n",
       "195135   21-30 Minutes                    752  Ipswich & East Suffolk   \n",
       "195136   16-20 Minutes                    786  Ipswich & East Suffolk   \n",
       "195137   31-60 Minutes                    456  Ipswich & East Suffolk   \n",
       "195138   11-15 Minutes                   1253  Ipswich & East Suffolk   \n",
       "\n",
       "             Date  \n",
       "195134 2021-12-01  \n",
       "195135 2021-12-01  \n",
       "195136 2021-12-01  \n",
       "195137 2021-12-01  \n",
       "195138 2021-12-01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "appointments_df = (\n",
    "    appointments_df # select only the columns we need, and remove unknown\n",
    "    .loc[(appointments_df['SUB_ICB_LOCATION_CODE'].isin(const.SUB_ICB_CODES.keys())) & (appointments_df['ACTUAL_DURATION'] != \"Unknown / Data Quality\")]\n",
    "    .assign(\n",
    "        Alliance=appointments_df['SUB_ICB_LOCATION_CODE'].map(\n",
    "            const.SUB_ICB_CODES), # add the alliance column\n",
    "        Date=pd.to_datetime(appointments_df['Appointment_Date'], format='%d%b%Y')) # convert the date column to datetime\n",
    "        # drop the columns we don't need\n",
    "    .drop(columns=['SUB_ICB_LOCATION_CODE', 'SUB_ICB_LOCATION_ONS_CODE', 'SUB_ICB_LOCATION_NAME', 'ICB_ONS_CODE', 'REGION_ONS_CODE', 'Appointment_Date']))\n",
    "appointments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Summary statistics\n",
    "\n",
    "1. Group by Alliance and FY\n",
    "2. Create left bin edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_binned_df = (appointments_df\n",
    "                          .groupby(['Alliance', pd.Grouper(key='Date',freq='BA-MAR',label='right'),'ACTUAL_DURATION'])\n",
    "                          .sum()\n",
    "                          .reset_index()\n",
    "                          .rename(columns={'Date':'financial_year'}).replace({\"1-5\":\"01-05\",\"6-10\":\"06-10\"})\n",
    "                          .assign(financial_year=lambda df: df['financial_year'].dt.year-1,\n",
    "                                  left_bin=lambda x: x.ACTUAL_DURATION.map(lambda x: int(x.split(\"-\")[0]))))\n",
    "\n",
    "appointments_binned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Convert to dictionaries & create continuous data\n",
    "1. Create empty dictionaries to be populated\n",
    "2. Pivot the data for easier conversion\n",
    "3. Populate the dictionaries with\n",
    "    - Binned data\n",
    "    - Continuous (Linear interpolation between bin edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create empty dicts in dicts to store the binned and continuous data\n",
    "binned_dict = {k: {k2: None for k2 in range(2021,2024)} for k in appointments_binned_df.Alliance.unique()}\n",
    "cont_dict = {k: {k2: None for k2 in range(2021,2024)} for k in appointments_binned_df.Alliance.unique()}\n",
    "\n",
    "# pivot the binned data\n",
    "pivot_bins_df = (appointments_binned_df\n",
    "                 .groupby(['Alliance','financial_year','left_bin']).sum() # sum the counts\n",
    "                 .reset_index() # reset the index\n",
    "                 .drop(columns=['ACTUAL_DURATION']) # drop the duration column\n",
    "                 .sort_values(by=['Alliance','financial_year','left_bin']) # sort the values\n",
    "                 .pivot(index=['Alliance','financial_year'],columns='left_bin',values='COUNT_OF_APPOINTMENTS')\n",
    "                 .reset_index() # reset the index \n",
    "                 .fillna(0) # fill the NaNs with 0 (there are none)\n",
    "                 .set_index(['Alliance','financial_year']) # set the index\n",
    "                 )\n",
    "\n",
    "# iterate through the pivot table and create the binned and continuous data\n",
    "for index, row in pivot_bins_df.iterrows():\n",
    "    # get the bin sums as np array\n",
    "    bin_sums = row.to_numpy()\n",
    "    # add to dictionary in correct place\n",
    "    binned_dict[index[0]][index[1]] = bin_sums\n",
    "    # create the continuous array (initially empty)\n",
    "    for binned_index, bin_count in enumerate(bin_sums):\n",
    "        # create the datapoints in bin\n",
    "        cont_array = np.linspace(start=HIST_BIN_EDGES[binned_index], stop=HIST_BIN_EDGES[binned_index+1], num=bin_count) \n",
    "        # if it is the first, instantiate the main array\n",
    "        if binned_index == 0:\n",
    "            cont_array_all = cont_array\n",
    "        else:\n",
    "            cont_array_all = np.append(cont_array_all, cont_array)\n",
    "    cont_dict[index[0]][index[1]] = cont_array_all \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "### Histograms with provided bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for alliance, dataset in cont_dict.items():\n",
    "    fig, axes = plt.subplots(1,3,figsize=(10,7.5))\n",
    "    ax_index = 0\n",
    "    for year, array in dataset.items():\n",
    "        ax = sns.histplot(array, kde=False, bins=HIST_BIN_EDGES, stat='probability', ax=axes[ax_index])\n",
    "        ax.yaxis.grid(True)\n",
    "        if ax_index == 0:\n",
    "            ax.set_title(f\"{alliance} {year}\")\n",
    "            ax.set_ylabel(\"Probability\")\n",
    "        else:\n",
    "            ax.set_title(f\"{year}\")\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax_index += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit distributions\n",
    "- Exponential\n",
    "- Lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loop over binned_dict and fit distributions\n",
    "fitted_distributions = {}\n",
    "for alliance, years in cont_dict.items():\n",
    "    if not fitted_distributions.get(alliance, None):\n",
    "        fitted_distributions[alliance] = {}# add dict entry\n",
    "    for year, array in years.items():\n",
    "        if array is not None:\n",
    "            # Fit an exponential distribution to the data using MLE\n",
    "            expon_fit_params = list(float(i) for i in stats.expon.fit(array))\n",
    "            lognorm_fit_params = list(float (i) for i in stats.lognorm.fit(array))\n",
    "            fitted_distributions[alliance][year] = {\"lognorm\":lognorm_fit_params, \"expon\":expon_fit_params}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 'Theoretical' distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alliance, years in fitted_distributions.items():\n",
    "    for year, params in years.items():\n",
    "        if year==2023:\n",
    "            lognorm_samples = stats.lognorm.rvs(*params['lognorm'], size=10000)\n",
    "            expon_samples = stats.expon.rvs(*params['expon'], size=10000)\n",
    "            fig, axes = plt.subplots(1,2,figsize=(10,7.5))\n",
    "            ax = sns.histplot(lognorm_samples, kde=False, bins=HIST_BIN_EDGES, stat='probability', ax=axes[0])\n",
    "            ax.yaxis.grid(True)\n",
    "            ax.set_title(f\"{alliance} {year} lognorm\")\n",
    "            ax.set_ylabel(\"Probability\")\n",
    "            ax = sns.histplot(expon_samples, kde=False, bins=HIST_BIN_EDGES, stat='probability', ax=axes[1])\n",
    "            ax.yaxis.grid(True)\n",
    "            ax.set_title(f\"{alliance} {year} expon\")\n",
    "            ax.set_ylabel(\"Probability\")\n",
    "plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to YAML\n",
    "#### Using 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only 2023 data\n",
    "distributions_2023 = {k: {dist: params for dist, params in v[2023].items()} for k,v in fitted_distributions.items()}\n",
    "# output to yaml\n",
    "with open(OUTPUT_YAML_FILE, 'w') as yaml_file:\n",
    "    yaml.dump(distributions_2023, yaml_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
