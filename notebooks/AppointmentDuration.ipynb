{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appointment Duration \n",
    "\n",
    "###  Aim: Make an approximation of appointment time by ICB and staff type that we can use in discrete event simulation?\n",
    "- Find all binned appointments by area\n",
    "- Fit a probability distribution function\n",
    "- Export to Yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a bit of a hack to get relative imports \n",
    "# to work as if these notebooks were in a package\n",
    "# change cwd to project root if 'notebooks' in PATH\n",
    "from os import chdir\n",
    "from pathlib import Path\n",
    "if 'notebooks' in str(Path.cwd()):\n",
    "    chdir('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# custom imports from src\n",
    "from src.schemas import DataCatalog\n",
    "import src.constants as const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_APPOINTMENTS_CATALOG_NAME:str = 'Appointments in General Practice, September 2023'\n",
    "HIST_BIN_EDGES = [1, 6, 11, 16, 21, 31, 60]\n",
    "OUTPUT_YAML_FILE = \"outputs/assumptions/appointment_duration.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from data catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_catalog = DataCatalog.load_from_yaml(\"data_catalog.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling\n",
    "### Part 1 - Cleaning\n",
    "1. remove unknown appointments\n",
    "2. Convert date columns\n",
    "3. Remove unrequired columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Appointments in General Practice, September 2023'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_catalog.get_catalog_entry_by_name(GP_APPOINTMENTS_CATALOG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_df = data_catalog.single_data_sources[GP_APPOINTMENTS_CATALOG_INDEX].load()\n",
    "\n",
    "appointments_df = (\n",
    "    appointments_df # select only the columns we need, and remove unknown\n",
    "    .loc[(appointments_df['SUB_ICB_LOCATION_CODE'].isin(const.SUB_ICB_CODES.keys())) & (appointments_df['ACTUAL_DURATION'] != \"Unknown / Data Quality\")]\n",
    "    .assign(\n",
    "        Alliance=appointments_df['SUB_ICB_LOCATION_CODE'].map(\n",
    "            const.SUB_ICB_CODES), # add the alliance column\n",
    "        Date=pd.to_datetime(appointments_df['Appointment_Date'], format='%d%b%Y')) # convert the date column to datetime\n",
    "        # drop the columns we don't need\n",
    "    .drop(columns=['SUB_ICB_LOCATION_CODE', 'SUB_ICB_LOCATION_ONS_CODE', 'SUB_ICB_LOCATION_NAME', 'ICB_ONS_CODE', 'REGION_ONS_CODE', 'Appointment_Date']))\n",
    "appointments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Summary statistics\n",
    "\n",
    "1. Group by Alliance and FY\n",
    "2. Create left bin edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_binned_df = (appointments_df\n",
    "                          .groupby(['Alliance', pd.Grouper(key='Date',freq='BA-MAR',label='right'),'ACTUAL_DURATION'])\n",
    "                          .sum()\n",
    "                          .reset_index()\n",
    "                          .rename(columns={'Date':'financial_year'}).replace({\"1-5\":\"01-05\",\"6-10\":\"06-10\"})\n",
    "                          .assign(financial_year=lambda df: df['financial_year'].dt.year-1,\n",
    "                                  left_bin=lambda x: x.ACTUAL_DURATION.map(lambda x: int(x.split(\"-\")[0]))))\n",
    "\n",
    "appointments_binned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Convert to dictionaries & create continuous data\n",
    "1. Create empty dictionaries to be populated\n",
    "2. Pivot the data for easier conversion\n",
    "3. Populate the dictionaries with\n",
    "    - Binned data\n",
    "    - Continuous (Linear interpolation between bin edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create empty dicts in dicts to store the binned and continuous data\n",
    "binned_dict = {k: {k2: None for k2 in range(2021,2024)} for k in appointments_binned_df.Alliance.unique()}\n",
    "cont_dict = {k: {k2: None for k2 in range(2021,2024)} for k in appointments_binned_df.Alliance.unique()}\n",
    "\n",
    "# pivot the binned data\n",
    "pivot_bins_df = (appointments_binned_df\n",
    "                 .groupby(['Alliance','financial_year','left_bin']).sum() # sum the counts\n",
    "                 .reset_index() # reset the index\n",
    "                 .drop(columns=['ACTUAL_DURATION']) # drop the duration column\n",
    "                 .sort_values(by=['Alliance','financial_year','left_bin']) # sort the values\n",
    "                 .pivot(index=['Alliance','financial_year'],columns='left_bin',values='COUNT_OF_APPOINTMENTS')\n",
    "                 .reset_index() # reset the index \n",
    "                 .fillna(0) # fill the NaNs with 0 (there are none)\n",
    "                 .set_index(['Alliance','financial_year']) # set the index\n",
    "                 )\n",
    "\n",
    "# iterate through the pivot table and create the binned and continuous data\n",
    "for index, row in pivot_bins_df.iterrows():\n",
    "    # get the bin sums as np array\n",
    "    bin_sums = row.to_numpy()\n",
    "    # add to dictionary in correct place\n",
    "    binned_dict[index[0]][index[1]] = bin_sums\n",
    "    # create the continuous array (initially empty)\n",
    "    for binned_index, bin_count in enumerate(bin_sums):\n",
    "        # create the datapoints in bin\n",
    "        cont_array = np.linspace(start=HIST_BIN_EDGES[binned_index], stop=HIST_BIN_EDGES[binned_index+1], num=bin_count) \n",
    "        # if it is the first, instantiate the main array\n",
    "        if binned_index == 0:\n",
    "            cont_array_all = cont_array\n",
    "        else:\n",
    "            cont_array_all = np.append(cont_array_all, cont_array)\n",
    "    cont_dict[index[0]][index[1]] = cont_array_all \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "### Histograms with provided bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for alliance, dataset in cont_dict.items():\n",
    "    fig, axes = plt.subplots(1,3,figsize=(10,7.5))\n",
    "    ax_index = 0\n",
    "    for year, array in dataset.items():\n",
    "        ax = sns.histplot(array, kde=False, bins=HIST_BIN_EDGES, stat='probability', ax=axes[ax_index])\n",
    "        ax.yaxis.grid(True)\n",
    "        if ax_index == 0:\n",
    "            ax.set_title(f\"{alliance} {year}\")\n",
    "            ax.set_ylabel(\"Probability\")\n",
    "        else:\n",
    "            ax.set_title(f\"{year}\")\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax_index += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit distributions\n",
    "- Exponential\n",
    "- Lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loop over binned_dict and fit distributions\n",
    "fitted_distributions = {}\n",
    "for alliance, years in cont_dict.items():\n",
    "    if not fitted_distributions.get(alliance, None):\n",
    "        fitted_distributions[alliance] = {}# add dict entry\n",
    "    for year, array in years.items():\n",
    "        if array is not None:\n",
    "            # Fit an exponential distribution to the data using MLE\n",
    "            expon_fit_params = list(float(i) for i in stats.expon.fit(array))\n",
    "            lognorm_fit_params = list(float (i) for i in stats.lognorm.fit(array))\n",
    "            fitted_distributions[alliance][year] = {\"lognorm\":lognorm_fit_params, \"expon\":expon_fit_params}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 'Theoretical' distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alliance, years in fitted_distributions.items():\n",
    "    for year, params in years.items():\n",
    "        if year==2023:\n",
    "            lognorm_samples = stats.lognorm.rvs(*params['lognorm'], size=10000)\n",
    "            expon_samples = stats.expon.rvs(*params['expon'], size=10000)\n",
    "            fig, axes = plt.subplots(1,2,figsize=(10,7.5))\n",
    "            ax = sns.histplot(lognorm_samples, kde=False, bins=HIST_BIN_EDGES, stat='probability', ax=axes[0])\n",
    "            ax.yaxis.grid(True)\n",
    "            ax.set_title(f\"{alliance} {year} lognorm\")\n",
    "            ax.set_ylabel(\"Probability\")\n",
    "            ax = sns.histplot(expon_samples, kde=False, bins=HIST_BIN_EDGES, stat='probability', ax=axes[1])\n",
    "            ax.yaxis.grid(True)\n",
    "            ax.set_title(f\"{alliance} {year} expon\")\n",
    "            ax.set_ylabel(\"Probability\")\n",
    "plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to YAML\n",
    "#### Using 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only 2023 data\n",
    "distributions_2023 = {k: {dist: params for dist, params in v[2023].items()} for k,v in fitted_distributions.items()}\n",
    "# output to yaml\n",
    "with open(OUTPUT_YAML_FILE, 'w') as yaml_file:\n",
    "    yaml.dump(distributions_2023, yaml_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
